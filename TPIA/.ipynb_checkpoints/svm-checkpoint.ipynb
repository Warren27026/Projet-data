{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd677599-45ca-484d-a2ad-57a19dda8468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The duration of FR_001.wav in seconds: 5.762\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "x,freq = librosa.load(\"Dataset/0002.wav\",sr=16000)\n",
    "print(\"The duration of FR_001.wav in seconds:\",len(x)/freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64008cb3-f551-4082-93fd-451aaf0b5d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 181)\n"
     ]
    }
   ],
   "source": [
    "# This function will return n_mfcc number of MFCC per\n",
    "#     a window of time in audio time series\n",
    "x_mfcc=librosa.feature.mfcc(y=x,sr=freq, n_mfcc=20)\n",
    "print(x_mfcc.shape)\n",
    "# x_mfcc is an array with 40 values for a window of time\n",
    "# The len(x_mfcc) is a proportion of wav file duration (5-6 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838cc59-e47e-4460-b1eb-6c68f7fee827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feature_extractor_1(audio_file_dir):\n",
    "\n",
    "    #load the audio files\n",
    "    x,freq = librosa.load(audio_file_dir,sr=16000)\n",
    "    #extract 20 MFCCs\n",
    "    mfcc=librosa.feature.mfcc(y=x,sr=freq,n_mfcc=20)\n",
    "    #calculate the mean and variance of each MFFC \n",
    "    mean_mfccs=np.mean(mfcc,axis=1)\n",
    "    var_mfccs=np.var(mfcc,axis=1)\n",
    "    #return mean and variance as the audio file feature \n",
    "    return list(mean_mfccs)+list(var_mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c769f0-d73b-4c36-ad54-8cfce6eaaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor_2(audio_file_dir):\n",
    "\n",
    "    #load the audio files\n",
    "    x,freq = librosa.load(audio_file_dir,sr=16000)\n",
    "    # trim the first 5 seconds (Sequence Truncation)\n",
    "    length_of_5seconds=5*16000\n",
    "    x_5sec=x[:length_of_5seconds]\n",
    "    # extract 20 MFCCs\n",
    "    mfccs_5sec=librosa.feature.mfcc(y = x_5sec,sr=freq,n_mfcc=20)\n",
    "    # return mfcc of the first 5 sec as the audio file feature\n",
    "    return mfccs_5sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770deaf-9559-476d-a23f-844ab90cd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor_2(audio_file_dir):\n",
    "    import librosa\n",
    "    import numpy as np\n",
    "\n",
    "    # Charger le fichier audio\n",
    "    x, freq = librosa.load(audio_file_dir, sr=16000)\n",
    "    \n",
    "    # Garder uniquement les 5 premi√®res secondes\n",
    "    length_of_5seconds = 5 * 16000\n",
    "    x_5sec = x[:length_of_5seconds]\n",
    "\n",
    "    # Extraire les MFCCs\n",
    "    mfccs_5sec = librosa.feature.mfcc(y=x_5sec, sr=freq, n_mfcc=20)\n",
    "\n",
    "    # Moyenne et variance des MFCCs\n",
    "    mean_mfccs = np.mean(mfccs_5sec, axis=1)\n",
    "    var_mfccs = np.var(mfccs_5sec, axis=1)\n",
    "\n",
    "    # Retourner un vecteur concat√©n√©\n",
    "    return list(mean_mfccs) + list(var_mfccs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7f74f0-d915-4ff7-9b7f-e5f5c5b0cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#set data_dir to the directory of your data files\n",
    "data_dir= \"Dataset/\"\n",
    "\n",
    "# Read file info file to get the list of audio files and their labels\n",
    "file_list=[]\n",
    "label_list=[]\n",
    "with open(data_dir+\"Info.txt\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        # The first column contains the file name\n",
    "        file_list.append(row[0])\n",
    "        # The last column contains the lable (language)\n",
    "        label_list.append(row[-1]) \n",
    "        \n",
    "        \n",
    "# create a dictionary for labels\n",
    "lang_dic={'EN':0,'FR':1,'AR':2,'JP':3}\n",
    "\n",
    "# create a list of extracted feature (MFCC) for files\n",
    "x_data=[]\n",
    "\n",
    "for audio_file in file_list:\n",
    "    #file_feature = feature_extractor_2(data_dir+audio_file)\n",
    "    file_feature = feature_extractor_1(data_dir+audio_file)\n",
    "    #add extracted feature to dataset \n",
    "    x_data.append(file_feature)\n",
    "\n",
    "# create a list of labels for files\n",
    "y_data=[]\n",
    "for lang_label in label_list:\n",
    "    #convert the label to a value in {0,1,2,3} as the class label\n",
    "    y_data.append(lang_dic[lang_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79fac9c2-a594-4183-a5c8-5d51c1731f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# shuffle two lists\n",
    "temp_list = list(zip(x_data, y_data))\n",
    "random.shuffle(temp_list)\n",
    "x_data, y_data = zip(*temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec467a21-0841-4004-b0eb-5e60bb9b3737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 930, in partial_fit\n    X = validate_data(\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1101, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. StandardScaler expected <= 2.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 4. GridSearchCV avec validation crois√©e\u001b[39;00m\n\u001b[0;32m     30\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     31\u001b[0m     pipeline,\n\u001b[0;32m     32\u001b[0m     param_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 5. R√©sultats\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Meilleurs hyperparam√®tres :\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    999\u001b[0m     )\n\u001b[1;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 930, in partial_fit\n    X = validate_data(\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"C:\\Users\\HP\\anaconda3\\envs\\tp_ia_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1101, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. StandardScaler expected <= 2.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Division du jeu de donn√©es\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data,\n",
    "    test_size=0.33,\n",
    "    shuffle=True,\n",
    "    stratify=y_data,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Pipeline : StandardScaler + SVC\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 3. Grille d'hyperparam√®tres √©tendue\n",
    "param_grid = {\n",
    "    'svm__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf'],\n",
    "    'svm__gamma': ['scale', 'auto', 0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# 4. GridSearchCV avec validation crois√©e\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. R√©sultats\n",
    "print(\"‚úÖ Meilleurs hyperparam√®tres :\", grid_search.best_params_)\n",
    "print(\"‚úÖ Score moyen en validation crois√©e :\", grid_search.best_score_)\n",
    "\n",
    "# 6. √âvaluation sur le jeu de test\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"üéØ Accuracy sur le jeu de test :\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüìä Rapport de classification :\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"EN\", \"FR\", \"AR\", \"JP\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d12b2-0e8b-44df-8d1f-ff7e17bc8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(grid_search.best_estimator_, \"best_svm_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cbda9-f3c3-47f2-83e0-fa79ee8c0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "#from feature_extractor import feature_extractor_1  # adapte si besoin\n",
    "\n",
    "# Dossier contenant les fichiers de test et Info.csv\n",
    "data_dir = \"Test_Set/\"\n",
    "output_filename = \"Benewinde_TP1_SVM_Version5\"\n",
    "\n",
    "# Chargement du mod√®le entra√Æn√© (Pipeline)\n",
    "model = load(\"best_svm_pipeline.joblib\")\n",
    "\n",
    "# Dictionnaires de correspondance\n",
    "class2lang_dic = {0: \"EN\", 1: \"FR\", 2: \"AR\", 3: \"JP\"}\n",
    "\n",
    "# Lecture du fichier Info.csv\n",
    "file_list = []\n",
    "with open(data_dir + \"Info.csv\", 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        file_list.append(row[0])  # nom du fichier audio\n",
    "\n",
    "# Fichier de sortie\n",
    "with open(data_dir + f\"{output_filename}.csv\", 'w') as f:\n",
    "    f.write(\"ID,Label\\n\")\n",
    "\n",
    "    for filename in file_list:\n",
    "        filepath = data_dir + filename\n",
    "        features = feature_extractor_2(filepath)  # ‚Üí vecteur numpy\n",
    "        features = np.array(features).reshape(1, -1)\n",
    "        predicted_class = model.predict(features)[0]\n",
    "        predicted_lang = class2lang_dic[predicted_class]\n",
    "\n",
    "        print(f\"{filename}: {predicted_lang}\")\n",
    "        f.write(f\"{filename},{predicted_lang}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c1c65-6083-4df9-89a9-515c2da7a574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
